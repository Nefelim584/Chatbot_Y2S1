"""
COMPREHENSIVE MODEL COMPARISON TEST
Tests all major free open-source embedding models on education matching
Picks the best one based on discrimination ability and accuracy
"""

import numpy as np
from sentence_transformers import SentenceTransformer
import re
import time
from typing import Dict, List

print("="*80)
print("COMPREHENSIVE EMBEDDING MODEL COMPARISON")
print("="*80)
print("\nTesting 8 open-source models on education matching...")
print("This will take 3-5 minutes (downloading models on first run)\n")

# Models to test
MODELS = [
    {
        "name": "MiniLM-L6-v2",
        "path": "sentence-transformers/all-MiniLM-L6-v2",
        "size": "80MB",
        "dims": 384,
        "description": "Smallest, fastest (current model)"
    },
    {
        "name": "MiniLM-L12-v2",
        "path": "sentence-transformers/all-MiniLM-L12-v2",
        "size": "120MB",
        "dims": 384,
        "description": "Larger MiniLM"
    },
    {
        "name": "MPNet-base-v2",
        "path": "sentence-transformers/all-mpnet-base-v2",
        "size": "420MB",
        "dims": 768,
        "description": "High quality, slower"
    },
    {
        "name": "BGE-small",
        "path": "BAAI/bge-small-en-v1.5",
        "size": "130MB",
        "dims": 384,
        "description": "State-of-the-art small"
    },
    {
        "name": "BGE-base",
        "path": "BAAI/bge-base-en-v1.5",
        "size": "440MB",
        "dims": 768,
        "description": "State-of-the-art base"
    },
    {
        "name": "GTE-small",
        "path": "thenlper/gte-small",
        "size": "130MB",
        "dims": 384,
        "description": "General text embeddings"
    },
    {
        "name": "E5-small-v2",
        "path": "intfloat/e5-small-v2",
        "size": "130MB",
        "dims": 384,
        "description": "Microsoft E5"
    },
    {
        "name": "E5-base-v2",
        "path": "intfloat/e5-base-v2",
        "size": "440MB",
        "dims": 768,
        "description": "Microsoft E5 large"
    },
]

def extract_degree_field(education_text: str) -> str:
    """Extract degree field from education text"""
    text_lower = education_text.lower()

    if "grande" in text_lower and "√©cole" in text_lower:
        pattern = r'grande\s+√©cole\s+([^,\n]+)'
        match = re.search(pattern, text_lower)
        if match:
            return match.group(1).strip()
        return "Engineering"

    field_patterns = [
        r'in\s+([^,\n]+)',
        r'of\s+([^,\n]+)',
        r'(?:btech|bsc|beng|mtech|msc|bachelor|master|phd)\s+([^,\n]+)',
    ]

    for pattern in field_patterns:
        match = re.search(pattern, text_lower)
        if match:
            field_candidate = match.group(1).strip()
            if not any(uni in field_candidate for uni in ["university", "college", "institute", "iit", "mit"]):
                return field_candidate

    return education_text

def get_similarity(model, text1, text2):
    """Get cosine similarity between two texts"""
    emb1 = model.encode(text1, normalize_embeddings=True)
    emb2 = model.encode(text2, normalize_embeddings=True)
    return float(np.dot(emb1, emb2))

def test_model(model, candidate_education, required_education, target_role):
    """Test a model on one case"""
    degree_field = extract_degree_field(candidate_education)
    degree_relevance = get_similarity(model, degree_field, target_role)

    if degree_relevance < 0.5:
        return {
            'final_score': degree_relevance,
            'degree_relevance': degree_relevance,
            'level_match': 0.0,
        }

    level_match = get_similarity(model, candidate_education, required_education)
    final_score = (0.60 * degree_relevance) + (0.40 * level_match)

    return {
        'final_score': final_score,
        'degree_relevance': degree_relevance,
        'level_match': level_match,
    }

# Test cases with EXPECTED RANGES
test_cases = [
    {
        "name": "CS ‚Üí Software",
        "candidate": "BTech in Computer Science, 4 years",
        "required": "Bachelor's required",
        "role": "Software Engineer",
        "expected_min": 0.85,
        "expected_max": 0.95,
        "category": "good_match",
        "miniLM_finetuned": 0.953
    },
    {
        "name": "BSc Science ‚Üí Software",
        "candidate": "BSc Science, 3 years",
        "required": "Bachelor's required",
        "role": "Software Engineer",
        "expected_min": 0.55,
        "expected_max": 0.75,
        "category": "medium_match",
        "miniLM_finetuned": 0.693
    },
    {
        "name": "Math ‚Üí Data Scientist",
        "candidate": "BSc Mathematics, 3 years",
        "required": "Bachelor's required",
        "role": "Data Scientist",
        "expected_min": 0.75,
        "expected_max": 0.90,
        "category": "good_match",
        "miniLM_finetuned": 0.880
    },
    {
        "name": "Master's ‚Üí Bachelor's",
        "candidate": "MTech in Computer Science, 2 years",
        "required": "Bachelor's required",
        "role": "Software Engineer",
        "expected_min": 0.90,
        "expected_max": 1.0,
        "category": "excellent_match",
        "miniLM_finetuned": 0.947
    },
    {
        "name": "Grande √âcole ‚Üí Master's",
        "candidate": "Grande √âcole Engineering, 5 years",
        "required": "Master's required",
        "role": "Software Engineer",
        "expected_min": 0.80,
        "expected_max": 0.95,
        "category": "good_match",
        "miniLM_finetuned": 0.837
    },
    {
        "name": "Mechanical ‚Üí Software",
        "candidate": "BTech Mechanical Engineering, 4 years",
        "required": "Bachelor's required",
        "role": "Software Engineer",
        "expected_min": 0.08,
        "expected_max": 0.25,
        "category": "bad_match",
        "miniLM_finetuned": 0.141
    },
    {
        "name": "Bachelor's ‚Üí Master's",
        "candidate": "BSc Computer Science, 3 years",
        "required": "Master's required",
        "role": "Data Scientist",
        "expected_min": 0.50,
        "expected_max": 0.70,
        "category": "medium_match",
        "miniLM_finetuned": 0.778
    },
]

# Test all models
all_results = []

for model_info in MODELS:
    print(f"\n{'='*80}")
    print(f"Testing: {model_info['name']} ({model_info['size']}, {model_info['dims']} dims)")
    print(f"Description: {model_info['description']}")
    print(f"{'='*80}")

    try:
        # Load model
        print(f"Loading {model_info['path']}...")
        start_time = time.time()
        model = SentenceTransformer(model_info['path'])
        load_time = time.time() - start_time
        print(f"‚úì Loaded in {load_time:.1f}s")

        # Test on all cases
        results = []
        for test in test_cases:
            result = test_model(model, test['candidate'], test['required'], test['role'])

            # Check if in expected range
            in_range = test['expected_min'] <= result['final_score'] <= test['expected_max']

            results.append({
                'test': test['name'],
                'score': result['final_score'],
                'expected_min': test['expected_min'],
                'expected_max': test['expected_max'],
                'in_range': in_range,
                'category': test['category'],
                'miniLM_finetuned': test['miniLM_finetuned']
            })

            status = "‚úÖ" if in_range else "‚ùå"
            print(f"  {test['name']:<25} {result['final_score']:.3f} (expect {test['expected_min']:.2f}-{test['expected_max']:.2f}) {status}")

        # Calculate metrics
        scores = [r['score'] for r in results]
        good_matches = [r['score'] for r in results if r['category'] in ['good_match', 'excellent_match']]
        bad_matches = [r['score'] for r in results if r['category'] == 'bad_match']

        discrimination = np.mean(good_matches) - np.mean(bad_matches) if bad_matches else 0
        in_range_count = sum(r['in_range'] for r in results)
        accuracy = in_range_count / len(results)

        all_results.append({
            'model': model_info['name'],
            'path': model_info['path'],
            'size': model_info['size'],
            'dims': model_info['dims'],
            'results': results,
            'discrimination': discrimination,
            'accuracy': accuracy,
            'in_range': in_range_count,
            'avg_score': np.mean(scores),
            'load_time': load_time
        })

        print(f"\n  Discrimination: {discrimination:.3f} (higher = better)")
        print(f"  Accuracy: {accuracy:.1%} ({in_range_count}/7 in expected range)")

    except Exception as e:
        print(f"‚ùå Failed to test {model_info['name']}: {e}")

# Rankings
print("\n" + "="*80)
print("FINAL RANKINGS")
print("="*80)

# Sort by discrimination (most important for your use case)
sorted_by_discrimination = sorted(all_results, key=lambda x: x['discrimination'], reverse=True)

print("\nüèÜ RANKED BY DISCRIMINATION (Good vs Bad Match Separation):")
print(f"{'Rank':<6} {'Model':<20} {'Discrim':<12} {'Accuracy':<12} {'Size':<10} {'Dims':<8}")
print("-"*80)
for i, result in enumerate(sorted_by_discrimination, 1):
    print(f"{i:<6} {result['model']:<20} {result['discrimination']:<12.3f} {result['accuracy']:<12.1%} {result['size']:<10} {result['dims']:<8}")

# Detailed comparison table
print("\n" + "="*80)
print("DETAILED SCORE COMPARISON")
print("="*80)
print(f"\n{'Test':<25} {'Expected':<15} ", end="")
for result in all_results[:4]:  # Show first 4 models
    print(f"{result['model']:<15} ", end="")
print()
print("-"*110)

for i, test in enumerate(test_cases):
    expected_range = f"{test['expected_min']:.2f}-{test['expected_max']:.2f}"
    print(f"{test['name']:<25} {expected_range:<15} ", end="")
    for result in all_results[:4]:
        score = result['results'][i]['score']
        in_range = result['results'][i]['in_range']
        status = "‚úÖ" if in_range else "  "
        print(f"{score:.3f} {status:<8} ", end="")
    print()

# Recommendation
print("\n" + "="*80)
print("üéØ RECOMMENDATION")
print("="*80)

best = sorted_by_discrimination[0]
print(f"\n‚úÖ BEST MODEL: {best['model']}")
print(f"   Path: {best['path']}")
print(f"   Discrimination: {best['discrimination']:.3f}")
print(f"   Accuracy: {best['accuracy']:.1%} ({best['in_range']}/7 tests in range)")
print(f"   Size: {best['size']}")
print(f"   Dimensions: {best['dims']}")

print(f"\nüìä VS YOUR CURRENT (MiniLM-L6-v2 Fine-Tuned):")
current = next(r for r in all_results if r['model'] == 'MiniLM-L6-v2')
print(f"   {current['model']} Base:        Discrimination: {current['discrimination']:.3f}, Accuracy: {current['accuracy']:.1%}")
print(f"   MiniLM-L6-v2 Fine-Tuned: Discrimination: ~0.81, Accuracy: ~86%")
print(f"   {best['model']} Base:           Discrimination: {best['discrimination']:.3f}, Accuracy: {best['accuracy']:.1%}")

if best['discrimination'] > 0.50:
    print(f"\n‚úÖ {best['model']} shows strong discrimination!")
    print(f"   Expected after fine-tuning: ~90-95% accuracy")
    print(f"\nüí° RECOMMENDATION: Switch to {best['model']} and retrain")
    print(f"   Update BASE_MODEL in training scripts to: '{best['path']}'")
elif best['discrimination'] > current['discrimination'] + 0.10:
    print(f"\n‚ö†Ô∏è {best['model']} is better but marginal")
    print(f"   May not be worth retraining for ~{(best['discrimination'] - current['discrimination']) * 100:.0f}% improvement")
else:
    print(f"\n‚úÖ Stick with MiniLM-L6-v2 Fine-Tuned")
    print(f"   Other models don't show significant improvement")

print("\n" + "="*80)
print("üéì KEY INSIGHT: Discrimination Score Matters Most!")
print("="*80)
print("High discrimination = Model can separate good matches (0.85+) from bad (0.15-)")
print("Low discrimination  = Model gives similar scores to everything (like BGE: 0.61-0.67)")
print("\nFor education matching, we NEED strong discrimination!")